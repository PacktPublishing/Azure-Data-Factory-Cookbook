{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory.models import *\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create unformation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_item(group):\n",
    "    \"\"\"Print an Azure object instance.\"\"\"\n",
    "    print(\"\\tName: {}\".format(group.name))\n",
    "    print(\"\\tId: {}\".format(group.id))\n",
    "    if hasattr(group, 'location'):\n",
    "        print(\"\\tLocation: {}\".format(group.location))\n",
    "    if hasattr(group, 'tags'):\n",
    "        print(\"\\tTags: {}\".format(group.tags))\n",
    "    if hasattr(group, 'properties'):\n",
    "        print_properties(group.properties)\n",
    "\n",
    "\n",
    "def print_properties(props):\n",
    "    \"\"\"Print a ResourceGroup properties instance.\"\"\"\n",
    "    if props and hasattr(props, 'provisioning_state') and props.provisioning_state:\n",
    "        print(\"\\tProperties:\")\n",
    "        print(\"\\t\\tProvisioning State: {}\".format(props.provisioning_state))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def print_activity_run_details(activity_run):\n",
    "    \"\"\"Print activity run details.\"\"\"\n",
    "    print(\"\\n\\tActivity run details\\n\")\n",
    "    print(\"\\tActivity run status: {}\".format(activity_run.status))\n",
    "    if activity_run.status == 'Succeeded':\n",
    "        print(\"\\tNumber of bytes read: {}\".format(activity_run.output['dataRead']))\n",
    "        print(\"\\tNumber of bytes written: {}\".format(activity_run.output['dataWritten']))\n",
    "        print(\"\\tCopy duration: {}\".format(activity_run.output['copyDuration']))\n",
    "    else:\n",
    "        print(\"\\tErrors: {}\".format(activity_run.error['message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Authentificate Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure subscription ID\n",
    "subscription_id = 'insert subscription id here'\n",
    "\n",
    "rg_name = 'ADFCookbook'\n",
    "\n",
    "# In a shared lab environment, such as XtremeLabs, make sure this is globally unique ex. add a number or your initials\n",
    "df_name = 'ADFCookbook-From-Python' \n",
    "\n",
    "credentials = ClientSecretCredential(\n",
    "        client_id='insert application (client) id',\n",
    "        client_secret='insert client secret value',\n",
    "        tenant_id='insert directory (tenant) id'\n",
    ")\n",
    "\n",
    "rg_params = {'location':'eastus'}\n",
    "df_params = {'location':'eastus'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_client = ResourceManagementClient(credentials, subscription_id)\n",
    "adf_client = DataFactoryManagementClient(credentials, subscription_id)\n",
    "df_resource = Factory(location='eastus')\n",
    "df = adf_client.factories.create_or_update(rg_name, df_name, df_resource)\n",
    "print_item(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Created a linked service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure Storage linked service\n",
    "ls_name = 'ADFCookbookLinkedServicePython'\n",
    "\n",
    "# IMPORTANT: specify the name and key of your Azure Storage account.\n",
    "# Optionally, override the full value with your connection string\n",
    "storage_string = SecureString(value='DefaultEndpointsProtocol=https;AccountName=insert account name here;AccountKey=insert account key here;EndpointSuffix=core.windows.net')\n",
    "\n",
    "ls_azure_storage = LinkedServiceResource(properties=AzureStorageLinkedService(connection_string=storage_string))\n",
    "ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)\n",
    "print_item(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'ADFCookbookDS-Input-Python'\n",
    "ds_ls = LinkedServiceReference(reference_name=ls_name)\n",
    "blob_path= 'adfcookbook/input'\n",
    "blob_filename = 'SalesOrders.txt'\n",
    "ds_azure_blob= DatasetResource(properties=AzureBlobDataset(linked_service_name=ds_ls, folder_path=blob_path, file_name = blob_filename))\n",
    "ds = adf_client.datasets.create_or_update(rg_name, df_name, ds_name, ds_azure_blob)\n",
    "print_item(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsOut_name = 'ADFCookbookDS-Output-Python'\n",
    "output_blobpath = 'adfcookbook/output'\n",
    "dsOut_azure_blob = DatasetResource(properties=AzureBlobDataset(linked_service_name=ds_ls, folder_path=output_blobpath))\n",
    "dsOut = adf_client.datasets.create_or_update(rg_name, df_name, dsOut_name, dsOut_azure_blob)\n",
    "print_item(dsOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy activity\n",
    "act_name = 'ADFCookbookCopyData'\n",
    "blob_source = BlobSource()\n",
    "blob_sink = BlobSink()\n",
    "dsin_ref = DatasetReference(reference_name=ds_name)\n",
    "dsOut_ref = DatasetReference(reference_name=dsOut_name)\n",
    "copy_activity = CopyActivity(name=act_name,inputs=[dsin_ref], outputs=[dsOut_ref], source=blob_source, sink=blob_sink)\n",
    "\n",
    "#Create a pipeline with the copy activity\n",
    "p_name = 'ADFCookbookCopyDataPipeline'\n",
    "params_for_pipeline = {}\n",
    "p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)\n",
    "p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)\n",
    "print_item(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_response = adf_client.pipelines.create_run(rg_name, df_name, p_name, parameters={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = adf_client.pipeline_runs.get(rg_name, df_name, run_response.run_id)\n",
    "print(\"\\n\\tPipeline run status: {}\".format(pipeline_run.status))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
